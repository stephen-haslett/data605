---
title: "DATA 605 Final Project"
author: "Stephen Haslett"
date: "12/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries.
library(data.table)
library(corrplot)
library(matrixcalc)
library(MASS)

# Disable scientific notation.
options(scipen = 999)
```
\ 

#### Problem 1

Using R, generate a random variable $X$ that has 10,000 random uniform numbers from $1$ to $N$, where $N$ can be any number of your choosing greater than or equal to $6$. Then generate a random variable $Y$ that has $10,000$ random normal numbers with a mean of $\mu =\sigma=(N+1)/2$.  

```{r, randomVariableX}
set.seed(100)

# Set N to any number greater than or equal to 6 (in this case, 8).
N <- 8

# Generate a random variable X that has 10,000 random uniform numbers from 1 to N.
X <- runif(10000, 1, N)

# Generate a random variable Y that has 10,000 random normal numbers with the requested mean.
Y <- rnorm(10000, mean = (N+1)/2, (N+1)/2)
```

\ 

##### _Problem 1 - Probability_

Calculate as a minimum the below probabilities $a$ through $c$.  Assume the small letter "$x$" is estimated as the median of the $X$ variable, and the small letter "$y$" is estimated as the 1st quartile of the $Y$ variable. Interpret the meaning of all probabilities.

```{r, probability}
# Small x is estimated as the median of the X variable.
x <- round(median(X), 2)
x

# Small y is estimated as the 1st quartile of the Y variable.
y <- round(quantile(Y, 0.25), 2)
y
```

**Answer**: Small x is equal to **4.48** (median of the X variable), small y is equal to **1.45** (1st quartile of the Y variable). 


A. $P(X>x \ | \ X>y)$

**Interpretation**:

_Probability that X is greater than its median given that X is greater than the first quartile of Y_.

$$P(X>x \ | \ X>y) = \frac{P(X>x \ , \ X>y)}{P(X>y)}$$

```{r, probabilityA}
# Define the events.
event_one <- (X > x)
event_two <- (X > y)

# P(X>x and X>y).
a_and_b <- length(X[event_one & event_two]) / length(X)

# P(X>y).
b <- length(X[event_two]) / length(X)

# P(X>x | X>y).
probability <- a_and_b / b

answer <- round(probability, 2)
answer
```

**Answer: $P(X > x \ | \ X > y)$ = 0.53**

\ 

B. $P(X>x, Y>y)$

**Interpretation**:

_Probability that $X$ is greater than $x$, and $Y$ is greater than $y$_. 

```{r, probabilityB}
# Define the events.
event_one <- (X > x)
event_two <- (Y > y)

# P(X > x).
X_gt_x <- length(X[event_one]) / length(X)

# P(Y > y).
Y_gt_y <- length(Y[event_two]) / length(Y)

probability  <- X_gt_x * Y_gt_y

answer <- round(probability, 2)
answer
```

**Answer: $P(X>x, Y>y)$ = 0.37**

\ 

C. $P(X<x \ | \ X>y)$

**Interpretation**:

_Probability that $X$ is less than its median given that it is greater than the first quantile of $Y$_.

```{r, probabilityC}
# Define the events.
event_one <- (X < x)
event_two <- (X > y)

# P(X > x & X > y).
a_and_b  <- length(X[event_one & event_two]) / length(X)

# P(X > y).
b <- length(X[event_two]) / length(X)

probability <- a_and_b / b 

answer <- round(probability, 2)
answer
```

**Answer: $P(X<x \ | \ X>y)$ = 0.47**

\ 

Investigate whether $P(X>x \ and \ Y>y) = P(X>x)P(Y>y)$ by building a table and evaluating the marginal and joint probabilities.

```{r, probabilityTable}


part_one <- (X > x)
prob_X_gt_x <- (length(part_one[part_one == TRUE])) / (length(part_one))

part_two <- (Y > y)
prob_Y_gt_y <- (length(part_two[part_two == TRUE])) / (length(part_two))

results_table <- data.table(
  Event = c('(X>x)', '(Y>y)', '(X>x)*(Y>y)', '(X>x and Y>y)'),
  Xx = c(prob_X_gt_x, prob_Y_gt_y, prob_X_gt_x * prob_Y_gt_y, prob_X_gt_x * prob_Y_gt_y),
  Yy = c(prob_Y_gt_y, prob_X_gt_x, prob_X_gt_x * prob_Y_gt_y, prob_X_gt_x * prob_Y_gt_y)
)

results_table
```

**Answer**: They are both equal.

\ 

#### Problem 2

Register for Kaggle.com and compete in the House Prices: Advanced Regression Techniques competition.

**Competition Objective**

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

\ 

**Import the training and test datasets and summarize them**.

```{r, loadData}
# Pull in the train and test datasets.
training_dataset <- read.csv('https://raw.githubusercontent.com/stephen-haslett/data605/data605-final-exam/train.csv')
test_dataset <- read.csv('https://raw.githubusercontent.com/stephen-haslett/data605/data605-final-exam/test.csv')
```

\ 

**Snapshot of the training dataset**.

```{r, trainingSnapshot}
head(training_dataset, 1)
```


**Summarize the training dataset**.

```{r, trainingSummary}
summary(training_dataset)
```

\ 

**Snapshot of the test dataset**.

```{r, testSnapshot}
head(test_dataset, 1)
```

**Summarize the test dataset**.

```{r, testSummary}
summary(test_dataset)
```

\ 

##### Descriptive and Inferential Statistics

**1. Provide univariate descriptive statistics and appropriate plots for the training data set**.

```{r, salesPrice}
# Summarize the training dataset's SalePrice variable.
summary(training_dataset$SalePrice)
```

\ 

**Create a histogram of sales prices**.

```{r, salesPricePlot}
hist(training_dataset$SalePrice,
     xlab = 'Sale Price',
     main = 'Distribution of Sales Prices',
     col = 'darkgreen')
```

\ 

**Create a QQ plot of sales prices**.

```{r, salesPriceQQPlot}
qqnorm(training_dataset$SalePrice, col = 'darkred')
qqline(training_dataset$SalePrice)
```

\ 

**2. Provide a scatterplot matrix for at least two of the independent variables and the dependent variable**.

```{r, scatterplotMatrix}
# Define the variables to include in the matrix.
sale_price <- training_dataset$SalePrice
lot_area <- training_dataset$LotArea
gr_liv_area <- training_dataset$GrLivArea
garage_area <- training_dataset$GarageArea

# Plot the matrix.
plot_data <- data.frame(sale_price, lot_area, gr_liv_area, garage_area)
pairs(plot_data, main = 'Scatterplot Matrix', col = '#50394c')
```

\ 

**3. Derive a correlation matrix for any three quantitative variables in the dataset**.

```{r, correlationMatrix}

# Create a dataframe containing the 3 variables to include in the matrix.
LotArea <- training_dataset$LotArea
GrLivArea <- training_dataset$GrLivArea
GarageArea <- training_dataset$GarageArea
matrix_variables <- data.frame(LotArea, GrLivArea, GarageArea)

# Create the correlation matrix.
cor_matrix <- cor(matrix_variables)
corrplot(cor_matrix, method = "shade")
```

\ 

**4. Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval**.

_4(a). Test LotArea Vs. GrLivArea_.

```{r, hypothesisTestingA}
# Test LotArea Vs. GrLivArea using the Pearson method with 80% confidence level.
cor.test(training_dataset$LotArea, training_dataset$GrLivArea, method = 'pearson', conf.level = 0.80)
```


_4(b). Test LotArea vs GarageArea_.

```{r, hypothesisTestingB}
# Test LotArea Vs. GarageArea vs with 80% confidence level.
cor.test(training_dataset$LotArea, training_dataset$GarageArea, method = 'pearson', conf.level = 0.80)
```


_4(c). Test GarageArea Vs. GrLivArea_.


```{r, hypothesisTestingC}
# Test GarageArea Vs. GrLivArea with 80% confidence level.
cor.test(training_dataset$GarageArea, training_dataset$GrLivArea, method = 'pearson', conf.level = 0.80)
```

\ 

**5. Discuss the meaning of your analysis. Would you be worried about familywise error? Why or why not**?

From the above results, the correlation between the selected variables is not equal to 0 in all 3 comparisons. Additionally, the p-values for all 3 samples are less than 0.05, so we can reject the null hypothesis.

Due to the fact that the correlation tests result in low p-values, I would not be worried about family-wise errors when measuring relationships across the 3 attributes.

\ 

##### Linear Algebra and Correlation

**1. Invert your correlation matrix from above**.

```{r, invertedMatrix}
# Invert the matrix.
inverted_matrix <- solve(cor_matrix)
round(inverted_matrix, 2)
```

\ 

**2. Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix**.

_2(a). Multiply the correlation matrix by the precision matrix_.

```{r, mutiplyCorPrecMatrices}
correlation_x_precision <- cor_matrix %*% inverted_matrix
round(correlation_x_precision, 2)
```


_2(b). Multiply the precision matrix by the correlation matrix_.

```{r, mutiplyPrecCorMatrices}
precision_x_correlation <- inverted_matrix %*% cor_matrix
round(precision_x_correlation, 2)
```

\ 

**3. Conduct LU decomposition on the matrix**.

```{r, luDecomposition}
# Perform LU decomposition on the matrix.
lu.decomposition(inverted_matrix)
```

\ 

##### Calculus-Based Probability & Statistics

**1. Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary. Then load the MASS package and run fitdistr to fit an exponential probability density function**.

```{r, Calculsus1}
# Select the right skewed TotalBsmtSF variable from the training dataset
# and run fitdistr to fit an exponential probability density function.
fit <- fitdistr(training_dataset$TotalBsmtSF, "exponential")
fit
```

** 2. Find the optimal value of $\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, $\lambda$))**.

```{r, Calculsus2A}
# Check the names that are available in the fit.
names(fit)
```

```{r, Calculsus2B}
lambda <- fit$estimate
samples <- rexp(1000, lambda)
```

Plot a histogram and compare it with a histogram of your original variable.

```{r, Calculsus2C}
# Histrgram of samples.
hist(samples, breaks = 100)
```


```{r, Calculsus2E}
# Histrgram of original variable.
original_variable <- training_dataset$TotalBsmtSF
hist(original_variable, breaks = 100)
```

**Conclusion:**

The histogram of samples produces a less skewed distribution than that of the original variable.


**3a. Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF)**.

```{r, pdf}
quantile(samples, probs = c(0.05, 0.95))
```


**3b. Also generate a 95% confidence interval from the empirical data, assuming normality**.

```{r, confidence}
empirical_data <- training_dataset$TotalBsmtSF
mean(empirical_data)

normality <-rnorm(length(empirical_data), mean(empirical_data), sd(empirical_data))
hist(normality)
```


**3c. Finally, provide the empirical 5th percentile and 95th percentile of the data**.

```{r, final}
quantile(normality, probs = c(0.05, 0.95))
```
